{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1\n",
    "### The K-Armed Bandit problem\n",
    "\n",
    "\n",
    "\n",
    "### Estimating action values\n",
    "\n",
    "\n",
    "### Exploration vs. exploitation tradeoff\n",
    "\n",
    "## Week 2\n",
    "### Markov decision process\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "\n",
    "### Continuing tasks\n",
    "\n",
    "\n",
    "## Week 3\n",
    "### Policies and value functions\n",
    "\n",
    "\n",
    "### Bellman equations\n",
    "\n",
    "\n",
    "### Optimality\n",
    "\n",
    "\n",
    "## Week 4\n",
    "### Dynamic programming\n",
    "\n",
    "\n",
    "### Policy evaluation (prediction)\n",
    "\n",
    "\n",
    "### Policy iteration (control)\n",
    "\n",
    "\n",
    "### Generalized policy iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Past notes\n",
    "\n",
    "Reinforcement learning introduction\n",
    "\n",
    "\t• State is the signal that the agent sense from the environment. Current state should contain all information needed for decision making - if the time or the last action needs to be known for current action, then that knowledge should be embed in state.\n",
    "\t• Reward signal is short term and occassional, value signal is general long term expectation of reward, or the expected fraction of total episode reward in that state (if total reward is 1, value function in a state is the probability of winning from that state). It is the expected (weighted average for stochasticity of policy or environment) reward in long run, when following a policy pi.\n",
    "\t• Policy is function that outputs action (or action distribution) given state.\n",
    "\t• A model tells the agent what next state and reward will be given a state and action.\n",
    "On top of it, you can apply approximation and abstraction like parameterized value function.\n",
    "\n",
    "3 fundamental class of RL method\n",
    "\t1. Dynamic programming: if exact environment transition is known, optimal value function and policy can be learned without interacting with environment.\n",
    "\t2. Monte Carlo: estimate value function from sampled episodes. It is low variance.\n",
    "\t3. Temporal Difference: learn value function by bootstrapping over experience. It is online and sample efficient.\n",
    "Monte Carlo and TD can be learned on/off policy. Different methods can be combined, and approximation can be applied on top of them for better models.\n",
    "\n",
    "4 fundamental RL challenge\n",
    "\t1. The representation\n",
    "\t2. Generalization\n",
    "\t3. Temporal credit assignment\n",
    "\t4. Exploration\n",
    "\n",
    "Multi-armed Bandits\n",
    "\n",
    "There is just 1 state, the action value function is: q(a) = expected return of that action. Update at each step:\n",
    "Action value=Current estimation+Step size×(Reward−Current estimation); the step size parameter denotes learning rate from new samples. Constant step size parameters don't converge, but allows value estimation to track nonstationary problems.\n",
    "\n",
    "Exploitation is selecting action with highest estimated reward.\n",
    "Exploration is selecting random/non-optimal actions to update action value estimations. Exploration can result in longer higher return.\n",
    "Balancing exploration and exploitation\n",
    "\t• Initializing values with high value.\n",
    "\t• Epsilon soft policy: in epsilon times, select random action\n",
    "\t• Upper Confidence Bound method - that prioritizes selection of actions that hasn’t been selected in a long time.\n",
    "\n",
    "Associative search/contextual bandits\n",
    "More than one situation, where the distribution of action values change. Then each setting should be associated with a state that the agent can detect. If the states themselves get affected by the actions the agent selects, then we have full RL problem.\n",
    "\n",
    "Markov Decision Process\n",
    "\n",
    "Actions: impact next state, thus also future rewards.\n",
    "\t\n",
    "Environment dynamics or transition function p: takes in state and action, and returns probability distribution of new states and rewards.\n",
    "\t\n",
    "Reward signal: only states what goal to achieve, not how to achieve it. Discount factor is added to reward signals so that the agent focuses more on immediate rewards. There is tradeoff of immediate and future reward. Overall return is discounted sum of current and all future rewards: G_t=∑24_(k=t+1)^T▒〖〖Discount factor〗^(k−(t+1))×R_k 〗\n",
    "\t\n",
    "Value function: expected-discounted-winning probability (for total reward of 1) of a state, or state-action pair. Bellman equation gives a way to compute by bootstrapping (from values of successor states).\n",
    "\t\t○ Value of state following a policy\n",
    "v(s)=∑8_a▒〖π(a|s)∑8_(s^′, r)▒〖p(s^′,r│s,a)[(r+γv_π (s^′ )]〗〗\n",
    "\t\t○ Value of state-action pair following a policy\n",
    "q(s,a)=∑8_(s^′, r)▒〖p(s^′,r│s,a)[(r+γv_π (s^′ )]〗=∑8_(s^′, r)▒〖p(s^′,r│s,a)[(r+γ(π(a′|s′)×q(s^′,a′))]〗\n",
    "\t\t○ Thus, v_π (s^′ )=π(a|s)×q(s′,a′)\n",
    "\t\t○ Given we know environment dynamics, the unknown in each states value function is the value of next state. So for n states, we'll have n equations. By solving them, we can get values of each states.\n",
    "\t\n",
    "Optimal policy: The number of possible policies are number of actions ^ number of states. An optimal policy is a deterministic policy that is at least as good as or better than all other policies in every states. There can be more than one optimal policy, and every optimal policy shares the same highest state value function.\n",
    "\t\n",
    "Optimal value function: The highest value you can get by taking optimal action from a state is the optimal value of that state. It's argmax value function\n",
    "v^∗ (s)=∑8_a▒〖π^∗ (a|s)∑8_(s^′, r)▒〖p(s^′,r│s,a)[(r+γv_π (s^′ )]〗〗\n",
    "\tNow, the optimal policy π^∗  is a deterministic policy - it assigns probability of 1 to the optimal action, and 0 to all other actions. so we can rewrite the function as:\n",
    "v^∗ (s)=max∑8_(s^′, r)▒〖p(s^′,r│s,a)[(r+γv_π (s^′ )]〗\n",
    "\tSimilarly for stat-action value:\n",
    "\tq^∗ (s,a)=∑8_(s^′, r)▒〖p(s^′,r│s,a)[(r+γ(maxq(s^′,a′))]〗\n",
    "Now, we can't compute optimal value function without optimal policy - so we can't just solve linear functions to get optimal value. But if we had optimal value function, then greedy policy (select the highest state-action value action) with respect to it would be optimal policy.\n",
    "\n",
    "Dynamic Programming\n",
    "\n",
    "We use dynamic programing (policy iteration: policy evaluation + policy improvement) to get optimal policy. We keep making policy 'strictly better' and better until it can't be improved no longer.\n",
    "\n",
    "Prediction is understanding performance of a given policy.\n",
    "Control is finding policy to maximize value function - so it does prediction, and updates policy based on prediction.\n",
    "\n",
    "Iterative policy evaluation is calculating value functions from a given policy. We start with arbitrary initialization for value function values, then for a random policy, continuous iterative calculation over all states (sweep) keeps updating the value function approximation of all states with respect to the policy. When the update will no longer be significant, we'll have the value function for the policy.\n",
    "\tWe can either keep two arrays to keep old and new values, or have just one array to store old and new values simultaneously.\n",
    "\n",
    "Policy improvement is using existing value function to determine better policy - by selecting greedy policy on the value function of previous policy . If a greedy policy on a value function is not different from the policy used to calculate the value function, then the prior policy is already optimal. Otherwise we'll get a policy that gets more rewards.\n",
    "\n",
    "Policy iteration: policy evaluation (approximating value function of random policy) > policy improvement (greedy policy over the value function) > (if policy had changed in the previous step) policy evaluation (approximating value function of the new policy) > … > optimal value function > optimal policy\n",
    "\tEither the value function is accurate but policy is not greedy, or the policy is greedy, but the value function is old. But with iteration, this gap reduced until convergence.\n",
    "\n",
    "Generalized policy iteration: instead of getting precise value function given a policy, we can approximate to some degree; instead of greedy policy, we can use somewhat greedy - and many more flexibility.\n",
    "\t• Value iteration: instead of finding value function of a given policy, performs just one sweep of value update, and take greedy policy on the approximate value function.\n",
    "\t• Asynchronous DP: sweeps are not done serially over all states, but important states are updated before/more.\n",
    "\n",
    "Bootstrapping is using values of successor states to compute value of a state.\n",
    "\n",
    "Curse of dimensionality: with more state and action variables, problem gets exponentially hard to solve.\n",
    "\n",
    "Monte Carlo\n",
    "\n",
    "Unlike DP, Monte Carlo estimates value function without environment transition probability - just take many samples and average. However, state values depend on policy - successor state-actions.\n",
    "In a sample run, list values of all states. In many samples, average values of each sets.\n",
    "\n",
    "Random start: Both state and state-action values are sampled to approximate average value. When following a policy to explore values in an episode, start of run is initialized in all possible state-actions with random start to ensure we've some estimation of many values.\n",
    "\n",
    "Monte Carlo generalized policy iteration:\n",
    "Policy evaluation: after an episode, update value estimation.\n",
    "Policy improvement: greedy policy on new state values\n",
    "\n",
    "Epsilon soft policy\n",
    "It might not always be possible to explore enough with explore start approach. Epsilon soft policy assigns probability of epsilon to taking random actions - which over time ensures exploration of all actions. But because of randomness, it doesn’t result in deterministic policy, rather learns optimal epsilon soft policy. \n",
    "Take greedy action (1−ε)  times, and random action ε/(A(s))  times, where the more an action gets selected in a state, the less likely it is to reselect it.\n",
    "\n",
    "Off policy learning\n",
    "Epsilon soft is neither good explorer or exploiter. Off policy learning solves exploration problem. It learns about a policy following a different policy. The target policy is π(a|s), we want to learn its value function. But the agent acts according to behavior policy b(a|s), which is an exploratory policy. The behavior policy must cover all the actions of target policy. Off policy learning can be done from previous data - such was expert input.\n",
    "\n",
    "Importance sampling\n",
    "We want to understand what reward might pi give with it's probability distribution of actions (target policy). But we'll use another probability distribution of actions (behavior policy). So, we can't simply take simple weighted average, we'll have to treat the reward signals.\n",
    "Expected reward under target policy = reward * importance sampling ratio (which is probability of target/behavior policy for selecting the action)\n",
    "\n",
    "Action probability\tb(x)\tpi(x)\tReward\tTimes sampled\n",
    "1\t.85\t.3\t1\t2\n",
    "2\t.05\t.1\t3\t1\n",
    "3\t\t\t\t0\n",
    "For a state with 3 actions, average expected reward with target policy = (1/3) * [1 * (.3/.85)] + [3 * (.1/.05)] + [1 * (.3/.85)] = 2.24 (real value is 2.2)\n",
    "\n",
    "Off policy Monte Carlo prediction\n",
    "v(s)=∑▒〖reward×(π(a|s)p(s^′,r│s,a))/(b(a|s)p(s^′,r│s,a) )=〗 ∑▒〖reward×(π(a|s))/(b(a|s))=∑▒〖reward×ρ〗〗\n",
    "Monte Carlo calculates from last reward to first, as rewards are received at the end of the episode. The ρ on each state is ρ of that state * ρ of successor state\n",
    "\n",
    "Extra: RL for human system\n",
    "Learning from old data means we can't check effectiveness of actions not selected. For example, lets say you've patient data one a particular sequence of treatment, and you want to explore effect of other sequences. In order to achieve generalization, causal reasoning and counterfactual or batch RL is used.\n",
    "We can use importance sampling to reweigh trajectories of a policy to learn about another policy, the model of the world isn't required as it cancels out. Parametric models is used with it.\n",
    "\n",
    "Temporal Difference\n",
    "\n",
    "TD learning is the core idea of RL. In Monte Carlo, we calculated value of a state as\n",
    "v(s)=estimated value+step size×(weighted sum of reward−estimated value). We needed to wait for the episode to end to calculate weighted sum of rewards from last step to first. But we can write:\n",
    "v(s)=estimated value+step size×(reward+discounted value of next state−estimated value); now we just need to wait for one step to update value. Here, the agent is predicting value, and updating the value after 1 time step.\n",
    "\n",
    "Generalized Policy Iteration with TD: SARSA\n",
    "SARSA prediction: learning state-action pair value with TD for policy evaluation in each step of an episode. And then doing policy improvement.\n",
    "\n",
    "SARSA is like the Bellman equation for state-action pair value.\n",
    "Q(S,A)←Q(S,A)+α(R+γQ(S^′,A^′ )−Q(S,A))\n",
    "\n",
    "Q learning is like Bellman optimality equation for learning state-action pair value.\n",
    "Q(S,A)←Q(S,A)+α(R+γmaxQ(S^′,A^′ )−Q(S,A))\n",
    "\n",
    "On policy SARSA vs off policy Q learning: On policy SARSA learns the difference of policies and their affect. Off policy  Q learning simply learns the optimal policy. Q learning is off policy without using importance sampling. Because unlike on policy SARSA which updates in terms of the next action of its policy, q learning updates the highest value next action, not one according to behavior policy. Q learnings behavior policy can be epsilon greedy, and target policy can be greedy with respect to state-value action value. Q learning doesn't need importance sampling to re-weigh it based on it's target policy. It simply associates weight of 1 with maximum reward action.\n",
    "\n",
    "Expected SARSA\n",
    "Unlike dynamic programming, that calculates estimated return by policy-weighted state-actions, Monte Carlo, TD (state) and SARSA (state-action) samples one state-action at a time. Expected SARSA uses its policy to probability weigh all actions. It computationally more expensive, but has lower variance. Expected SARSA is more robust than SARSA for large step sizes.\n",
    "Q(S,A)←Q(S,A)+α(R+γπ(a|s)Q(S^′,A^′ )−Q(S,A))\n",
    "\n",
    "Generality of expected SARSA\n",
    "If expected SARSA assigns probability of 1 to highest action-state pair, it become q learning. So q learning is a special case of expected SARSA.\n",
    "SARSA learns action values based on the current policy (on policy), q learning learns the optimal policy (off policy), expected SARSA can learn any policy (on or off policy).\n",
    "\n",
    "SARSA is sample based version of policy iteration. Q learning is sample based version of value iteration (sweep once and make greedy). Q learning makes the value function approach optimality, as long as it keeps exploring.\n",
    "TD is for value prediction given a policy - addresses prediction problem. SARSA, Q learning and Expected SARSA does both prediction and policy improvement - addresses control problem.\n",
    "\n",
    "Planning and Learning\n",
    "\n",
    "Model is transition probability: Given state-action, return probability distribution of reward and next states - p(s^′,r│s,a)\n",
    "\n",
    "Sample model: approximates by generating sample sequence, computationally inexpensive\n",
    "Distribution: create tree like distribution. Gives us more information about exact joint probability and variance, but is computationally expensive.\n",
    "Planning: Update value from stimulated experience. It improves sample efficiency. Example: one step tabular q planning. Planning dramatically speeds up learning. But random planning steps might not learn a lot at start.\n",
    "\n",
    "Dyna: from experience, update q values and learn model. Search control is then used to generate experience from the model, which helps update q value.\n",
    "\n",
    "Tabular Dyna-Q\n",
    "Model learning: For deterministic environment, model learning is just remembering new state-reward for state-action.\n",
    "Search control: for a number of times (randomly pick a state-action the agent has experienced, take a number of steps, update q value).\n",
    "\n",
    "Inaccurate model: Incomplete model, stochastic or changed environment\n",
    "Explore-exploit: the agent needs to take advantage of model to get more rewards - exploitation. But at the same time it needs to correct model by revisiting states not visited in a long time - exploration.\n",
    "Dyna-Q+ encourages exploration of states not visited in long time.\n",
    "\n",
    "Exhaustive search model like Monte Carlo Tree Search combines distribution and sample method. It samples to find high value states, and then searches through distribution.\n",
    "\n",
    "Prediction and control with function\n",
    "\n",
    "Parameterized function\n",
    "In house rent prediction problem, we can either learn value for all house (tabular method), or we can make house rent parameterized function - where we assign a parameter weight against an attribute of house (e.g. floorspace). Thus, if we learn a good weight, we'll be able to generalize estimation of rent v-hat (s, w) - a single weight can represent and change rent of many house.\n",
    "\n",
    "If there are more than one feature, then all of them are from feature vector X, and similarly multiple weights are from weight vector W. With linear function of weighted features, we can only accurately express values that change linearly with respect to features.\n",
    "\n",
    "Tabular value function is a part of generalized parameterized function, where each state has an associated feature and thus an associated weight. For a state s(k), the feature vector assigns 1 with f(k) feature, and 0 with all other features. Thus, only weight(k) gets updated for that state. Thus we'll have a table of state with value the value of the state(k) = weight(k) * 1.\n",
    "\n",
    "Generalization and discrimination\n",
    "Generalization in case of policy evaluation: updating value of one state impact other states. Discrimination is being able to identify difference.\n",
    "\n",
    "\n",
    "Supervised learning for value estimation\n",
    "While supervised estimated are designed to work with ground truth from start, but state value function can be estimated with supervised learning approximators that can deal with gradually updating (online), temporally correlated data, and bootstrapping (update depends on own estimation).\n",
    "\n",
    "Objective: Value error\n",
    "Mean squared error: Fraction of time spent in that state * [v(s) - v-hat(s, w)]^2 can be used to get difference of actual value function and parameterized approximation value function. It is a function of weights (it changes with v-hat, and v-hat changes with weights). If we want to minimize objective at a point where derivative is positive, we've to decrease the weight and vice versa. We'll change w to descent gradient until we reach local minimum. Since we'll be representing many states with a single weight, we've to avoid overfitting to a particular state.\n",
    "Even if we reach global minimum, it's just an approximation of actual value of state with lowest error - the accuracy depends on nature of parameterization and objective.\n",
    "\n",
    "Gradient descent\n",
    "Derivative of value function with respect to weights indicates change of f due to change of w (+/- for increase/decrease and magnitude). We can visualize a 2D line for expressing change of f with respect to a single weight. But if f changes with respect to many weights of a weight vector, we express the change as partial derivative of f with respect to the weights, and we use multidimensional gradient to illustrate the change. Notation: inverse-delta J(w) is partial derivative of objective with respect to weight.\n",
    "\n",
    "Gradient Monte Carlo for estimating v-hat\n",
    "The gradient of the objective is the gradient of value estimate - since only value estimate changes with change of w. If the estimator is a linear function, then it's derivative is simply the feature vector of the state.\n",
    "We'll then weight the error with this gradient and subtract it from current estimate.\n",
    "Mean squared error requires calculation of error of all states, instead we'll approximate with stochastic gradient descent.\n",
    "\n",
    "Stochastic gradient descent: we'll sample states following a policy, and reduce objective for that state. This can sometimes increase overall objective, but overall it'll reduce it.\n",
    "But since we don't have access to actual value of state following policy, we'll use estimated value instead. So at the end of episode, we'll calculate estimated value of state, and then perform stochastic gradient descent for each state.\n",
    "Next weight = weight + step size [v_pi(s,a)[sampled value] - v_hat(state, weight)[parameterized value estimation]] * gradient (v-hat)\n",
    "\n",
    "State aggregation with Monte Carlo\n",
    "Treat many similar states as same (like your Nature idea!). This is a linear value approximation case - a feature is associated with each group; if a state falls in that group, then value of feature is 1, otherwise 0; approximate value of the state is the weight associated with the state; and the gradient is the feature vector - thus stochastic gradient descent step only updates associated weight.\n",
    "\n",
    "Semi gradient TD for policy evaluation\n",
    "Unlike Monte Carlo - where we minimized error of sampled value and estimated value, TD minimizes error of estimation of next state (bootstrapped to get value of this state) and estimation of this state. Because of this, though TD updates value estimation with time, it might not converge to local minima of MSE, but it has lower variance and thus converges faster than Monte Carlo.\n",
    "We call this semi-gradient, since the gradient of error includes both estimations, as both of them changes with weight.\n",
    "\n",
    "Comparing TD with Monte Carlo with State Aggregation\n",
    "TD is sample efficient, but Monte Carlo is more accurate with more training.\n",
    "\n",
    "Linear function approximation\n",
    "For linear function, updating weight vector = old weight vector + step size * TD error * feature vector (for linear function, gradient of TD error is feature vector, since estimated value is product of feature and weight vector). With well designed features using domain knowledge (e.g. floor space for rent), we can improve value update.\n",
    "\n",
    "Linear approximation is generalization of both tabular and state aggregation TD. For tabular TD, for a specific weight w(k) is assigned for state s(k), where only feature f(k) is assigned to 1 a and rest are assigned to 0.\n",
    "\n",
    "w(k) = w(k) + step size * TD error; which is the equation of tabular TD\n",
    "We can show the same for state aggregation TD.\n",
    "\n",
    "True objective for TD: DO MATH from Book\n",
    "We mentioned that Semi gradient TD was approximation of MSE convergence objective, but actually it converges to TD fixed point.\n",
    "The linear solution of weight vector converges to Fixed Point - which is linked to Bellman's equation. The relationship of fixed point and MSE depends on quality of feature and discount factor. If the value approximator of next state will good, then TD converges to MSE objective.\n",
    "\n",
    "Coarse coding\n",
    "State aggregation doesn't allow overlapping, coarse coding does. Coarse coding with many shapes can result in better discrimination, while less shapes can result in better generalization.\n",
    "\n",
    "Tile coding\n",
    "Is a type of coarse coding. Many grids of large squares achieves both good generalization and discrimination quickly. For high dimension state, each dimension can be tiled separately.\n",
    "\n",
    "Tile coding in TD\n",
    "Compared to total squares, number of active squares will be small. Only active squares will have assigned value 1. So instead of product of weight and feature vector, we can simply add relevant weights. The number of weights to add will always be the same, since number of active squares will be same.\n",
    "\n",
    "Relation of tile coding and NN?\n",
    "\n",
    "Neural Network are example of nonlinear function approximation, where each connection is a real value weight, and where layers and nodes can exponentially increase possible combinations of weight-features.\n",
    "\n",
    "NNs are nonlinear function that can build state features. Deep layers build on more complex abstraction. Bottleneck NN only focuses on most important features.\n",
    "\n",
    "Input layer takes vector s, which is multiplied by weight vector A to hidden layers feature vector x, which is multiplied by weight vector B to output layers vector y.\n",
    "\n",
    "Weight vector = w - alpha * partial derivative of w * input of that layer\n",
    "Partial derivative of predecessor weight depends on partial derivative of successor state.\n",
    "Optimization: Momentum and variable step size.\n",
    "\n",
    "DO DERIVATION\n",
    "\n",
    "David Silver\n",
    "DNN\tObjective\n",
    "Policy\tPolicy gradient\n",
    "Value\tTD error\n",
    "Model\tNext step prediction error\n",
    "We use backpropagation (change value of weights according to partial derivatives) to minimize objective.\n",
    "\n",
    "Control with function approximation with GPI: Episodic SARSA\n",
    "In prediction task, the features represented only states. But for control task (SARSA, Q learning), feature have to represent state-action. Actions require separate feature, and state features are stacked for each actions. Only features of a particular action will stay active (rest will be assign 0) when doing dot product.\n",
    "In NN, the output layer corresponds to action values, and last hidden layer state approximation.\n",
    "\n",
    "Expected SARSA and Q\n",
    "As Q values are estimated, w will be added parameter with s and a. Expected SARSA weights q values with policy expectation, and q learning acts greedily. Gradient term is added with both for weight update.\n",
    "\n",
    "Exploration in approximation\n",
    "In tabular case, optimistic values worked well, but it doesn't with NNs. Epsilon soft approach randomly explores. Systematic exploration with NN is an open research topic.\n",
    "\n",
    "Average and differential reward for continuous learning\n",
    "In continuous task, step size should be high to prioritize future rewards, but high step size means sum will approach infinity. Another way to deal with this is to calculate average reward.\n",
    "In this setting, action value is differential return - the difference of reward of an state-action and the average reward following a policy.\n",
    "Policies are compared with average reward, individual actions are compared with differential reward.\n",
    "Differential reward can be used in Bellman equations, there is no discounting in these equations.\n",
    "\n",
    "q(s,a)=E_π (G_t |S_t=s,A_t=a)=∑8_(s^′, r)▒〖p(s^′,r│s,a)[(r−r(π)+∑_a′▒〖π(a′|s′)×q(s^′,a′)〗]〗\n",
    "\n",
    "In sample based methods, the agents keeps updating estimation of average reward.\n",
    "\n",
    "Learning policy directly\n",
    "We can parameterize policy directly, and one way to do it is SoftMax approximation, where each action is given a probability of selection of 0-1, where sum of all action probability is 1.\n",
    "Learning policy can often be less complicated, can autonomously decrease exploration and converge to deterministic policy, stochastic policy can be parameterized better, and often the optimal policy is a stochastic one.\n",
    "\n",
    "Objective for learning policies\n",
    "Up until now, we've used generalized policy iteration to get policy form greedifying improving value function. Now we'll directly learn policy for optimizing average reward objective:\n",
    "r(π)=∑8_s▒〖μ(s)∑_a▒〖π(a|s,θ)∑8_(s^′, r)▒〖p(s^′,r│s,a) 〗〗〗\n",
    "The last part is probability weighted reward, over all possible action - weighted by parameterized policy probability, all such value is weighted by their time spent in that state-action.\n",
    "\n",
    "Policy gradient Theorem\n",
    "Gradient ascent: To maximize the objective, we'll take gradient of r(π)  with respect to policy parameter θ, and adjust θ to the direction to gradient ascent to get more reward.\n",
    "∇r(π)=∑8_s▒〖μ(s)∑_a▒〖∇π(a│s,θ) q_π (s,a) 〗〗\n",
    "∇π(a│s,θ)  is the gradient of policy with respect to θ - which gives us gradient of each action. Updating θ towards gradient of a particular action will increase the probability of selecting that action, at cost of decreased probability of selecting other actions. When we weight the gradient with state-action values of each action, the gradient changes away from lower towards highest action value. This change is then summed over each states. And we're not taking gradient of μ(s), which is hard to compute in policy optimization case.\n",
    "\n",
    "Sample based estimation of policy gradient\n",
    "From stochastic samples, we'll calculate stochastic gradient descent.\n",
    "∑8_s▒〖μ(s)∑_a▒〖∇π(a│s,θ) q_π (s,a) 〗〗  this was our equation. To take samples according to policy, we'll add π(a│s,θ)  to the equation.\n",
    "r(π)=∑8_s▒〖μ(s)∑_a▒〖π(a│s,θ)  1/(π(a│s,θ) ) ∇π(a│s,θ) q_π (s,a) 〗〗\n",
    " ∑8_s▒〖μ(s)∑_a▒〖π(a│s,θ) 〗〗  this entire part follows a policy distribution, and can be thought as expectation (in this case, sampled state-action gradient) under policy π.\n",
    "Thus, \n",
    "r(π)=E_π [1/(π(a│s,θ) ) ∇π(a│s,θ) q_π (s,a)]\n",
    "Thus, stochastic gradient ascent update:\n",
    "θ_(t+1)≐θ_t+α(∇π(a│s,θ))/(π(a│s,θ) ) q_π (s,a)≐θ_t+α∇lnπ(a│s,θ) q_π (s,a)\n",
    "Since we know the policy, we can calculate gradient ∇lnπ(a│s,θ). And since we can compute action values q_π (s,a)  with prediction algorithms like TD.\n",
    "\n",
    "Actor Critic\n",
    "We can write the policy update equation as\n",
    "θ_t+α∇lnπ(a│s,θ)∗TD Error\n",
    "The average reward TD error is the critic that judges value of an action, and the policy gradient part is the actor that updates policy.\n",
    "General framework:\n",
    "Sample action from policy, observe next state and reward.\n",
    "Evaluate state-action value with differential reward TD\n",
    "Update average reward\n",
    "Update weight parameter by value gradient\n",
    "Update policy with policy gradient\n",
    "There can be different combinations of function approximation and policy parameterization techniques. One can be linear function approximation and SoftMax policy gradient. We'll feed updating policy parameter to SoftMax function to update action preference.\n",
    "Instead of SoftMax, we can use Gaussian policies for continuous action.\n",
    "\n",
    "Psychology\n",
    "Cognitive maps\n",
    "Model (state transition and reward upon action). Cognitive map is like the state transition model of the environment, that can be learned without reward with supervised learning, but which can be used to plan to reach goal.\n",
    "\n",
    "Habitual and goal directed behavior\n",
    "Habitual: automated upon trigger. Slow to change.\n",
    "Goal directed: purposeful - controlled by knowledge of value of goals and understanding of world causal relation. Quick to change. Changes to habitual behavior upon repetition, as uncertainty regarding causal relation reduces with experience.\n",
    "\n",
    "Summary\n",
    "Prediction: classical conditioning\n",
    "Control: instrumental conditioning\n",
    "\n",
    "Frontiers\n",
    "General value functions and auxiliary tasks\n",
    "General value function or forecast:\n",
    "\t1. Off policy: value not dependent on policy\n",
    "\t2. Termination function: different discount rate - amount of reward in arbitrary state horizon\n",
    "\t3. Generalize beyond rewards, allow any arbitrary signal or 'cumulant'\n",
    "In addition to learning policy to maximize reward, other signals can be maximized to solve 'auxiliary tasks'. Learning to solve auxiliary tasks can help agent learn better representations.\n",
    "\n",
    "Temporal abstraction via options\n",
    "Enabling action decision of different temporal level with options with policy and termination function. Low level actions can terminate after one time step, high level actions can execute for many. Option value functions can be applied for basic or hierarchical policy.\n",
    "Bellman equation and dynamic programming can be applied for options set Ω(s).\n",
    "Options model can be learned by formulating it as collection of GVFs. By matching the policy, termination of GVFs and options, separate GVFs for reward and transition probability can be learned.\n",
    "\n",
    "Observations and state\n",
    "Since full state isn't always available, the agent will have access to observations, which a part of state, where reward is a part of the observation (e.g. a value in a vector of observation).\n",
    "\n",
    "If we consider specific stream of actions and observation as history, then state is summary of the history that is just as useful for predicting the future. Thus, state is function of history, S=f(H), and has Markov property for predictability. Two summary of history is same, if their output prediction is same. And the summary gives probability of any transition.\n",
    "\n",
    "POMDP deals with partially observable states.\n",
    "\n",
    "Predictive State Representations approaches, semantics state is grounded in prediction of future observations and actions. Some developed systems in this area are Observable Operator Models and Sequential Systems (Thon, 2017).\n",
    "\n",
    "The most promising idea is to let the agent learn many different predictions, which should lead to good state representation learning. But that'll require general language of prediction.\n",
    "\n",
    "Designing reward signals\n",
    "Reward is more like internal signal, which has evolved for long time in animals. Simple tasks can be taught with straightforward reward signal, but they can be hard for complex behavior.\n",
    "\n",
    "Giving sub-rewards can also be problematic, as the agent might maximize sub-goal without progressing. Instead, augmenting initial estimation of value function that might finally result can work.\n",
    "\n",
    "Shaping: train agent to solve easier problems, then challenging with related one step harder problem at a time.\n",
    "\n",
    "Imitation learning: given expert behavior, the agent tries to learn reward function driving the expert.\n",
    "\n",
    "Optimizing bilevel reward: giving candidate reward space, the RL agent tries some reward, learning ones performing best in terms of high level objective. Low level objectives can be gradient of high level one, like evolutionary survival. Even without reward function, agents can learn to solve same task in constrained environment (Singh, Lewis, Sorg, Barto have done some work in this field).\n",
    "\n",
    "Intrinsically motivated RL: for self development\n",
    "\n",
    "Remaining issues\n",
    "\t1. Current methods aren't good online learners, they forget old learning in case of new learnings.\n",
    "\t2. Representation learning, constructive induction, meta learning: not just learn a good function for this task, also allow better future learning.\n",
    "\t3. Model based method with approximation and attention\n",
    "\t4. Automating choice of tasks to learn to develop competencies: a full hierarchy of questions will be necessary for getting full hierarchy of answers\n",
    "\t5. Intrinsic curiosity: let the agent learn new skills that can be reused\n",
    "\t6. Safe deployment of RL in real-world systems\n",
    "\n",
    "Book 2\n",
    "\n",
    "4. Learning and using models\n",
    "What is a model\n",
    "All the info needed for simulating outcome of action - including full probability distribution in stochastic environment, or, for generative models, provide sample over next states.\n",
    "It can be a tabular case, it can be trained as generalized supervised learning process.\n",
    "\n",
    "5. Transfer in RL\n",
    "Inductive bias is the initial output assumption of new input. Inductive bias can be thought as purpose of transfer learning, which will be suitable for related tasks where knowledge is retained and transferable.\n",
    "Transfer learning has been used in supervised problems. Although RL learns to solve problems from scratch without supervision, transfer can improve sample efficiency and accuracy in related problems.\n",
    "\n",
    "Framework\n",
    "\n",
    "\n",
    "State space * Action space is domain of the task\n",
    "Transition * Reward is objective of the task\n",
    "Environment is space of tasks, and their probability distribution Ω. The tasks are drawn from environment to the learner.\n",
    "Knowledge - all inputs while solving task (prior expert knowledge, transfer knowledge, direct knowledge: instances or samples, representation, parameters), is input of a learning algorithm, which outputs possible solution as hypothesis. Thus, general learner is a mapper from knowledge to hypothesis.\n",
    "\n",
    "Taxonomy\n",
    "\n",
    "Method: source to target with fixed state-action space\n",
    "\n",
    "Method: across tasks with fixed state-action space\n",
    "\n",
    "Method: source to target with different state-action space\n",
    "\n",
    "Open questions\n",
    "\n",
    "8. Relational and first order logical MDP\n",
    "Sequential decisions in relational worlds\n",
    "\n",
    "MDPs with objects and relations\n",
    "\n",
    "Model based solutions\n",
    "\n",
    "Model, hierarchies and bias\n",
    "\n",
    "Developments, outlook\n",
    "\n",
    "9. Hierarchical approaches\n",
    "Introduction\n",
    "\n",
    "Approaches\n",
    "\n",
    "Learning structure\n",
    "\n",
    "Developments and summary\n",
    "\n",
    "10. Evolutionary computation for RL\n",
    "\n",
    "\n",
    "11. Model based Bayesian RL\n",
    "\n",
    "\n",
    "13. Predictively defined representations of State\n",
    "Introduction\n",
    "\n",
    "PSR\n",
    "\n",
    "Learning PSR model\n",
    "\n",
    "Planning with PSRs\n",
    "\n",
    "Models\n",
    "\n",
    "Conclusion\n",
    "\n",
    "\n",
    "19. Interesting directions and experts\n",
    "\n",
    "Extra\n",
    "\n",
    "Intrinsic reward\n",
    "Reward is both preference of agent designer, and parameter of agents behavior.\n",
    "\n",
    "Policy gradient can be used to update intrinsic reward.\n",
    "\n",
    "Abstraction and knowledge with RL\n",
    "Procedural knowledge: policies, skills, behavior.\n",
    "Predictive knowledge: value function, models - what might happen.\n",
    "This knowledge should be learnable from data, expressive (should represent many ideas), composable. Done with 'state abstraction, function approximation'.\n",
    "Temporal abstraction: breaking task in reasonable parts. This is formalized with 3 parts option: initiation, policy execution, termination. The option model has expected reward and transition model.\n",
    "Open question\n",
    "How to do temporal and state abstraction together: find a good feature scale and a good temporal scale for both modeling and control. Instead of thinking about small space, reason about airport; and instead of muscle twitches, reason about walking. How to design harmonious state and temporal abstraction is open research problem - graph hierarchy.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
