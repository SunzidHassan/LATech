{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Book\n",
    "\n",
    "### 1. Introduction\n",
    "**Deep learning**: building complicated concepts from simple ones - the hierarchical graph is deep.\n",
    "\n",
    "Knowledge base approach vs machine learning approach (AI systems acquire their own knowledge, by extracting patterns in raw data).\n",
    "\n",
    "**Features and Representation**: we provide simple ML algorithms a good representation (collection of features) of the independent variable. Like providing area of an apartment instead of pictures of it to a price estimator.\n",
    "\n",
    "**Representation learning**: not only learn the mapping from representation to output, but also the representation itself.\n",
    "\n",
    "**Autoencoder**: encoder function converts input data into a different representation, decoder function converts new representation back to the original format.\n",
    "\n",
    "In feature selection, we want to seperate **factors of variation** that explains the observed data. These may be unobserved or imagined. It can be hard to disentangle these factors of variation from the raw data.\n",
    "\n",
    "**Deep learning** helps address this central problem in representation learning by introducing representations that are expressed in terms of other, simpler representations.\n",
    "\n",
    "#### 1.21 Neural networks\n",
    "\n",
    "Neuroscience has given us a reason to hope that a single deep learning algorithm can solve many different tasks. Neuroscientists have found that ferrets can learn to “see” with the auditory processing region of their brain if their brains are rewired to send visual signals to that area (Von Melchner et al., 2000). This suggests that much of the mammalian brain might use a single algorithm to solve most of the different tasks that the brain solves.\n",
    "\n",
    "Most neural networks today are based on a model neuron called the **rectified linear unit**. However, deep learning research is not an attempt to simulate the brain. **Computational neuroscience** attempts to understand how brain works.\n",
    "\n",
    "Two important concepts of connectionism: **distributed representation** - each input to a system should be represented by many features, and each feature should be involved in the representation of many possible inputs (e.g., red-blue-green ~ trucks-cars-birds); and back-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## I Applied Math and Machine Learning Basics\n",
    "### 2. Linear Algebra\n",
    "#### 2.1 Scalars, Vectors, Matrices and Tensors\n",
    "**Scalers**: are single numbers, written with lowercase italics typeface, with their type, e.g., ``Let $s\\epsilon\\mathbf{R}$ be the slope of the line.\n",
    "\n",
    "**Vectors**: array of numbers, written with lowercase bold italics typeface, with elements written with italics with subscript, e.g., \n",
    "$\\textit{\\textbf{x}}=\n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_n\\\\\n",
    "\\end{array}\n",
    "\\right]$\n",
    "\n",
    "**Matrices**: 2-D array of numbers, written with uppercase bold typeface. For a real-valued matrix with height of $m$ and width of $n$, $\\mathbf{A}\\epsilon\\mathbf{R}^{m\\times n}$. $A_{i,j}$ indicates the element in $i$-th row and $j$-th column.\n",
    "\n",
    "**Tensors**: array with more than two axes.\n",
    "\n",
    "**Transpose**: operation that takes mirror image of a matrix across a diagonal line.\n",
    "\n",
    "#### 2.2 Multiplying Matrices and Vectors\n",
    "$\\mathbf{A}_{m\\times n}\\times \\mathbf{B}_{n\\times p}=\\mathbf{C}_{m\\times p}$\n",
    "\n",
    "Matrix product operations are distributive - $A(B+C)=AB+AC$, and associative - $A(BC)=(AB)C$, but not commutative - $AB=BA$ does not always hold. However, dot product between two vectors is commutative - $x^Ty=y^Tx$\n",
    "\n",
    "#### 2.3 Identity and Inverse Matrices\n",
    "#### 2.4 Linear Dependence and Span\n",
    "A square matrix with linearly dependent columns is known as singular.\n",
    "\n",
    "#### 2.5 Norms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Probability and Information Theory\n",
    "Probability theory allows making uncertain statements and reason in the presence of uncertainty. Information theory allows quantification of the amount of uncertainty in a probability distribution.\n",
    "\n",
    "**Frequentist probability** indicates outcome if the same state was repeated infinitely. **Bayesian probability** indicates degree of belief.\n",
    "\n",
    "#### 3.3 Probability Distributions\n",
    "Description of how likely a random variable or set of random variables is to take on each of its possible states. It depends on whether the variables are discrete or continuous.\n",
    "\n",
    "If $\\mathrm{x}$ follows probability mass function $P(\\mathrm{x})$, it's written as $\\mathrm{x}\\sim P(\\mathrm{x})$.\n",
    "\n",
    "**Joint probability distribution** $P(\\mathrm{x}=x,\\mathrm{y}=y)$.\n",
    "\n",
    "$\\sum_{x\\epsilon \\mathrm{x}}P(x)=1$ is **normalized**.\n",
    "\n",
    "**Probability mass function (PMF)** for discrete variables and **Probability density function (PDF)** for continuous variables.\n",
    "\n",
    "#### 3.4 Marginal Probability\n",
    "Probability distribution over a subset is known as marginal probability. For example, we know $P(\\mathrm{x,y})$, then we can find $P(\\mathrm{x})$ with the **sum rule**: $$\\forall x\\epsilon\\mathrm{x},P(\\mathrm{x}=x)=\\underset{y}{\\sum}P(\\mathrm{x}=x,\\mathrm{y}=y)$$\n",
    "In case of continuous variables, $$p(x)=\\int p(x,y)dy$$\n",
    "\n",
    "#### 3.5 Conditional Probability\n",
    "$$P(\\mathrm{y}=y|\\mathrm{x}=x)=\\frac{P(\\mathrm{y}=y,\\mathrm{x}=x)}{P(\\mathrm{x}=x)}$$\n",
    "\n",
    "What would happen if an action were undertaken is **intervention query**.\n",
    "\n",
    "#### 3.6 The Chain Rule of Conditional Probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Numeric Computation\n",
    "\n",
    "\n",
    "### 5. Machine Learning Basics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Deep Networks\n",
    "### 6. Deep Feedforward Networks\n",
    "\n",
    "\n",
    "### 7. Regularization for Deep Learning\n",
    "\n",
    "\n",
    "### 8. Optimization for Training Deep Models\n",
    "\n",
    "\n",
    "### 9. Convolutional Networks\n",
    "\n",
    "\n",
    "### 10. Sequence Modeling: Recurrent and Recursive Nets\n",
    "\n",
    "\n",
    "### 11. Practical Methodology\n",
    "\n",
    "\n",
    "### 12. Applications\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III Deep Learning Research\n",
    "### 13. Linear Factor Models\n",
    "\n",
    "\n",
    "### 14. Autoencoders\n",
    "\n",
    "\n",
    "### 15. Representation Learning\n",
    "\n",
    "\n",
    "### 16. Structured Probabilistic Models for Deep Learning\n",
    "\n",
    "\n",
    "### 17. Monte Carlo Methods\n",
    "\n",
    "\n",
    "### 18. Confronting the Partition Function\n",
    "\n",
    "\n",
    "### 19. Approximate Inference\n",
    "\n",
    "\n",
    "### 20. Deep Generative Models\n",
    "\n",
    "## "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
